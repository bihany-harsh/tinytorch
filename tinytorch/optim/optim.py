# base class for all optimizers
from ..tensor import Tensor
from typing import List, Tuple, OrderedDict, Any, Dict, Generator
import numpy as np

# took inspiration from toy code generated by Claude Sonnet 4.5
class Optimizer:
    def __init__(self, parameters, defaults: Dict[str, Any]):
        """
        Args:
            parameters: List[Tensor] | Tensor | param_groups
            defaults: dict, default hyperparameters for the optimizer
        """
        object.__setattr__(self, "_optim_state_params", OrderedDict())
        self.defaults = defaults # these are the default values of the hyperparameters
        self.state = {}
        self.param_groups = []
        
        if isinstance(parameters, Tensor):
            parameters = [parameters]
        
        if isinstance(parameters, (List, Tuple)):
            if len(parameters) == 0:
                raise ValueError("Optimizer got an empty parameter list, no parameters to optimize")
            
            # manage param groups
            if isinstance(parameters[0], Dict):
                for param_group in parameters:
                    self.add_param_group(param_group)
            else:
                self.add_param_group({"params": parameters})
        elif isinstance(parameters, Generator):
            parameters = [p for p in parameters]
            self.add_param_group({"params": parameters})
        else:
            raise ValueError("parameters argument given to the optimizer should be a Tensor, list/tuple of Tensors or list of param groups")
        
    def add_param_group(self, param_group):
        if not isinstance(param_group, Dict):
            raise ValueError("param_group to be added should be a dictionary")
        if "params" not in param_group:
            raise ValueError("param_group must contain the key 'params'")
        params = param_group["params"]
        if isinstance(params, Tensor):
            param_group["params"] = [params]
        elif isinstance(params, (List, Tuple)):
            param_group["params"] = list(params) # making it a list, if it were a tuple
        elif isinstance(params, Generator):
            param_group["params"] = [p for p in params]
        else:
            raise ValueError("params should be a Tensor | list/tuple of Tensor")
        
        # to set optimizer param values if not present
        for key, default_val in self.defaults.items():
            param_group.setdefault(key, default_val)
            
        self.param_groups.append(param_group)
            
    def __setattr__(self, name, value):
        if name in ["param_groups", "state", "defaults"]:
            object.__setattr__(self, name, value)
        else:
            self._optim_state_params[name] = value
        
    def __getattr__(self, name):
        object.__getattribute__(self, name)

    def zero_grad(self):
        for group in self.param_groups:
            for param in group["params"]:
                if not getattr(param, "requires_grad", True):
                    continue
                if param.grad is not None:
                    param.grad = np.zeros_like(param.grad)

    def step(self):
        raise NotImplementedError
    
    def state_dict(self):
        return {
            "state": self.state,
            "param_groups": self.param_groups,
        }

    def load_state_dict(self, state_dict):
        print("Optimizer::load_state_dict warnings: in case of missing values, defaults would follow")
        unexpected = []
        
        for key, value in state_dict.items():
            if not hasattr(self, key):
                unexpected.append(key)
            else:
                object.__setattr__(self, key, value)
                
        if unexpected:
            print(f"Optimizer::load_state_dict warnings\n unexpected: {unexpected}")
            
    def __repr__(self):
        main_str = self.__class__.__name__ + "(\n"
        
        for i, group in enumerate(self.param_groups):
            main_str += f"Parameter Group {i}\n"
            for key, value in group.items():
                if key != "params":
                    main_str += f"  {key}: {value}\n"
        main_str += ")"
        return main_str

class SGD(Optimizer):
    def __init__(
        self,
        parameters,
        lr: float = 0.01,
        momentum: float = 0.0,
        dampening: float = 0.0,
        weight_decay: float = 0.0,
        nesterov: bool = False,
        maximize: bool = False,
    ):
        
        # checks from Claude Sonnet 4.5
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= momentum < 1.0:
            raise ValueError(f"Invalid momentum value: {momentum}")
        if not 0.0 <= dampening <= 1.0:
            raise ValueError(f"Invalid dampening value: {dampening}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
        
        defaults = {
            "lr": lr,
            "momentum": momentum,
            "dampening": dampening,
            "weight_decay": weight_decay,
            "nesterov": nesterov,
            "maximize": maximize
        }
        
        super().__init__(parameters, defaults)
        
        for group in self.param_groups:
            for param in group["params"]:
                if getattr(param, "requires_grad", True):
                    self.state[id(param)] = {
                        # id(.) gives the address of the object and hence a unique identifier
                        "velocity": np.zeros_like(param.data)
                    }
    
    def step(self):
        
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group['momentum']
            dampening = group['dampening']
            weight_decay = group['weight_decay']
            nesterov = group['nesterov']
            maximize = group['maximize']
            
            for param in group["params"]:
                if not getattr(param, "requires_grad", True):
                    continue
                if param.grad is None:
                    raise ValueError("Parameter {param} does not have a gradient")
                
                grad = param.grad
                
                if maximize:
                    grad = -grad
                # regularizer
                if weight_decay != 0.0:
                    grad += weight_decay * param.data
                # momentum update
                if momentum != 0:
                    param_state = self.state[id(param)]
                    param_state["velocity"] = momentum * param_state["velocity"] + (1-dampening)*grad
                    if nesterov:
                        grad += momentum * param_state["velocity"]
                    else:
                        grad = param_state["velocity"]
                param.data -= lr * grad
            
# based on edits above, the following is restructured using ChatGPT 5
class RMSprop(Optimizer):
    def __init__(
        self,
        parameters,
        lr: float = 0.01,
        alpha: float = 0.99,
        eps: float = 1e-8,
        weight_decay: float = 0.0,
        momentum: float = 0.0,
        centered: bool = False,
        maximize: bool = False,
    ):
        # Parameter validation
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= alpha <= 1.0:
            raise ValueError(f"Invalid alpha value: {alpha}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
        if not 0.0 <= momentum:
            raise ValueError(f"Invalid momentum value: {momentum}")
        
        defaults = {
            "lr": lr,
            "alpha": alpha,
            "eps": eps,
            "weight_decay": weight_decay,
            "momentum": momentum,
            "centered": centered,
            "maximize": maximize
        }
        
        super().__init__(parameters, defaults)
        
        # Initialize state for each parameter
        for group in self.param_groups:
            for param in group["params"]:
                if getattr(param, "requires_grad", True):
                    state = {
                        "square_avg": np.zeros_like(param.data)
                    }
                    if momentum > 0:
                        state["momentum_buffer"] = np.zeros_like(param.data)
                    if centered:
                        state["grad_avg"] = np.zeros_like(param.data)
                    
                    self.state[id(param)] = state
    
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            alpha = group["alpha"]
            eps = group["eps"]
            weight_decay = group["weight_decay"]
            momentum = group["momentum"]
            centered = group["centered"]
            maximize = group["maximize"]
            
            for param in group["params"]:
                if not getattr(param, "requires_grad", True):
                    continue
                
                if param.grad is None:
                    raise ValueError(f"Parameter {param} does not have gradient")
                
                grad = param.grad
                
                # handle maximize (gradient ascent)
                if maximize:
                    grad = -grad
                
                # apply weight decay
                if weight_decay != 0:
                    grad = grad + weight_decay * param.data
                
                param_state = self.state[id(param)]
                
                # update running average of squared gradients
                param_state["square_avg"] = (
                    alpha * param_state["square_avg"] + (1 - alpha) * (grad ** 2)
                )
                
                # denominator (variance estimate)
                avg = param_state["square_avg"]
                
                if centered:
                    # update grad moving average
                    param_state["grad_avg"] = (
                        alpha * param_state["grad_avg"] + (1 - alpha) * grad
                    )
                    avg = avg - param_state["grad_avg"] ** 2
                
                denom = (avg + eps)**(0.5)  # sqrt
                
                if momentum > 0:
                    param_state["momentum_buffer"] = (
                        momentum * param_state["momentum_buffer"] + grad / denom
                    )
                    update = param_state["momentum_buffer"]
                else:
                    update = grad / denom
                
                # parameter update
                param.data -= lr * update


class Adam(Optimizer):
    def __init__(
        self,
        parameters,
        lr: float = 0.001,
        betas: Tuple[float, float] = (0.9, 0.999),
        eps: float = 1e-8,
        weight_decay: float = 0.0,
        amsgrad: bool = False,
        maximize: bool = False,
    ):
        # Parameter validation
        if not 0.0 <= lr:
            raise ValueError(f"Invalid learning rate: {lr}")
        if not 0.0 <= betas[0] < 1.0:
            raise ValueError(f"Invalid beta1 value: {betas[0]}")
        if not 0.0 <= betas[1] < 1.0:
            raise ValueError(f"Invalid beta2 value: {betas[1]}")
        if not 0.0 <= eps:
            raise ValueError(f"Invalid epsilon value: {eps}")
        if not 0.0 <= weight_decay:
            raise ValueError(f"Invalid weight_decay value: {weight_decay}")
        
        defaults = {
            "lr": lr,
            "betas": betas,
            "eps": eps,
            "weight_decay": weight_decay,
            "amsgrad": amsgrad,
            "maximize": maximize
        }
        
        super().__init__(parameters, defaults)
        
        # Initialize state for each parameter
        for group in self.param_groups:
            for param in group["params"]:
                if getattr(param, "requires_grad", True):
                    state = {
                        "step": 0,
                        "m": np.zeros_like(param.data),
                        "v": np.zeros_like(param.data)
                    }
                    if amsgrad:
                        state["v_max"] = np.zeros_like(param.data)
                    
                    self.state[id(param)] = state
    
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            beta1, beta2 = group["betas"]
            eps = group["eps"]
            weight_decay = group["weight_decay"]
            amsgrad = group["amsgrad"]
            maximize = group["maximize"]
            
            for param in group["params"]:
                if not getattr(param, "requires_grad", True):
                    continue
                
                if param.grad is None:
                    raise ValueError(f"Parameter {param} does not have gradient")
                
                grad = param.grad
                
                # handle maximize (gradient ascent)
                if maximize:
                    grad = -grad
                
                # weight decay
                if weight_decay != 0:
                    grad = grad + weight_decay * param.data
                
                param_state = self.state[id(param)]
                
                # increment step counter
                param_state["step"] += 1
                
                # update biased first moment estimate
                param_state["m"] = beta1 * param_state["m"] + (1 - beta1) * grad
                
                # update biased second moment estimate
                param_state["v"] = beta2 * param_state["v"] + (1 - beta2) * (grad ** 2)
                
                # bias correction
                m_hat = param_state["m"] / (1 - beta1 ** param_state["step"])
                v_hat = param_state["v"] / (1 - beta2 ** param_state["step"])
                
                if amsgrad:
                    # maintain max of v
                    param_state["v_max"] = np.maximum(param_state["v_max"], param_state["v"])
                    v_hat = param_state["v_max"] / (1 - beta2 ** param_state["step"])
                
                # parameter update
                param.data -= lr * m_hat / (np.sqrt(v_hat) + eps)
