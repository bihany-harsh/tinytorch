{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:03:11.919556Z",
     "start_time": "2023-07-10T08:03:11.072654Z"
    }
   },
   "outputs": [],
   "source": [
    "import tinytorch.tensor as tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "import tinytorch.optim as optim\n",
    "import tinytorch.nn.functional as F\n",
    "import tinytorch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "x = tensor.Tensor([[1, 1], [2, 2], [3, 3]])\n",
    "torch_t1 = torch.tensor([[1, 1], [2, 2], [3, 3]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.softmax(x, dim=1).data, torch.nn.functional.softmax(torch_t1, dim=1).data.numpy()\n",
    ")\n",
    "\n",
    "# testing log_softmax function\n",
    "x = tensor.Tensor([[0.2, 0.3], [0.4, 0.5], [0.6, 0.7]])\n",
    "torch_t1 = torch.tensor([[0.2, 0.3], [0.4, 0.5], [0.6, 0.7]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.log_softmax(x, dim=1).data,\n",
    "    torch.nn.functional.log_softmax(torch_t1, dim=1).data.numpy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:03:27.729469Z",
     "start_time": "2023-07-10T08:03:27.728572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# testing L1 loss function\n",
    "x = tensor.Tensor([[1, 3, 6], [2, 4, 5]])\n",
    "y = tensor.Tensor([[2, 4, 6], [3, 5, 7]])\n",
    "torch_t1 = torch.tensor([[1, 3, 6], [2, 4, 5]], dtype=torch.float64)\n",
    "torch_t2 = torch.tensor([[2, 4, 6], [3, 5, 7]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.l1_loss(x, y).data, torch.nn.functional.l1_loss(torch_t1, torch_t2).data.numpy()\n",
    ")\n",
    "\n",
    "# testing MSE loss function\n",
    "x = tensor.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "y = tensor.Tensor([[2, 3], [4, 5], [6, 7]])\n",
    "torch_t1 = torch.tensor([[1, 2], [3, 4], [5, 6]], dtype=torch.float64)\n",
    "torch_t2 = torch.tensor([[2, 3], [4, 5], [6, 7]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.mse_loss(x, y).data, torch.nn.functional.mse_loss(torch_t1, torch_t2).data.numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:03:42.123278Z",
     "start_time": "2023-07-10T08:03:42.120060Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# testing NLL loss function\n",
    "x = tensor.Tensor([[1, 3, 6], [2, 4, 5]])\n",
    "y = tensor.Tensor([0, 1])\n",
    "log_probs = F.log_softmax(x, dim=1)\n",
    "torch_t1 = torch.tensor([[1, 3, 6], [2, 4, 5]], dtype=torch.float64)\n",
    "torch_t2 = torch.tensor([0, 1], dtype=torch.int64)\n",
    "torch_log_probs = torch.nn.functional.log_softmax(torch_t1, dim=1)\n",
    "assert np.allclose(\n",
    "    F.nll_loss(log_probs, y).data,\n",
    "    torch.nn.functional.nll_loss(torch_log_probs, torch_t2).data.numpy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:04:00.769225Z",
     "start_time": "2023-07-10T08:04:00.768030Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# testing Cross Entropy loss function\n",
    "x = tensor.Tensor([[1, 3, 6], [2, 4, 5]])\n",
    "y = tensor.Tensor([0, 1])\n",
    "torch_t1 = torch.tensor([[1, 3, 6], [2, 4, 5]], dtype=torch.float64)\n",
    "torch_t2 = torch.tensor([0, 1], dtype=torch.int64)\n",
    "assert np.allclose(\n",
    "    F.cross_entropy(x, y).data,\n",
    "    torch.nn.functional.cross_entropy(torch_t1, torch_t2).data.numpy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:04:13.551169Z",
     "start_time": "2023-07-10T08:04:13.549293Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# testing softmax function on 3D data\n",
    "x = tensor.Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "torch_t1 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.softmax(x, dim=1).data, torch.nn.functional.softmax(torch_t1, dim=1).data.numpy()\n",
    ")\n",
    "\n",
    "# testing log_softmax function on 3D data\n",
    "x = tensor.Tensor([[[0.2, 0.3], [0.4, 0.5]], [[0.6, 0.7], [0.8, 0.9]]])\n",
    "torch_t1 = torch.tensor(\n",
    "    [[[0.2, 0.3], [0.4, 0.5]], [[0.6, 0.7], [0.8, 0.9]]], dtype=torch.float64\n",
    ")\n",
    "assert np.allclose(\n",
    "    F.log_softmax(x, dim=1).data,\n",
    "    torch.nn.functional.log_softmax(torch_t1, dim=1).data.numpy(),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:04:26.751418Z",
     "start_time": "2023-07-10T08:04:26.748543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# testing L1 loss function on 3D data\n",
    "x = tensor.Tensor([[[1, 3], [2, 4]], [[5, 7], [6, 8]]])\n",
    "y = tensor.Tensor([[[2, 4], [3, 5]], [[6, 8], [7, 9]]])\n",
    "torch_t1 = torch.tensor([[[1, 3], [2, 4]], [[5, 7], [6, 8]]], dtype=torch.float64)\n",
    "torch_t2 = torch.tensor([[[2, 4], [3, 5]], [[6, 8], [7, 9]]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.l1_loss(x, y).data, torch.nn.functional.l1_loss(torch_t1, torch_t2).data.numpy()\n",
    ")\n",
    "\n",
    "# testing MSE loss function on 3D data\n",
    "x = tensor.Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "y = tensor.Tensor([[[2, 3], [4, 5]], [[6, 7], [8, 9]]])\n",
    "torch_t1 = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=torch.float64)\n",
    "torch_t2 = torch.tensor([[[2, 3], [4, 5]], [[6, 7], [8, 9]]], dtype=torch.float64)\n",
    "assert np.allclose(\n",
    "    F.mse_loss(x, y).data, torch.nn.functional.mse_loss(torch_t1, torch_t2).data.numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:04:45.923026Z",
     "start_time": "2023-07-10T08:04:45.915163Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# backprop over MSE loss\n",
    "torch_t1 = torch.tensor(\n",
    "    [[1, 2], [3, 4], [5, 6]], dtype=torch.float64, requires_grad=True\n",
    ")\n",
    "torch_t2 = torch.tensor([[2, 3], [4, 5], [6, 7]], dtype=torch.float64)\n",
    "torch_loss = torch.nn.functional.mse_loss(torch_t1, torch_t2, reduction=\"mean\")\n",
    "torch_loss.backward()\n",
    "\n",
    "x = tensor.Tensor([[1, 2], [3, 4], [5, 6]], requires_grad=True, dtype=np.float64)\n",
    "y = tensor.Tensor([[2, 3], [4, 5], [6, 7]])\n",
    "loss = F.mse_loss(x, y)\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad.data, torch_t1.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:04:55.269266Z",
     "start_time": "2023-07-10T08:04:55.246090Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# backprop over NLL loss\n",
    "torch_t1 = torch.tensor(\n",
    "    [[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], dtype=torch.float64, requires_grad=True\n",
    ")\n",
    "torch_t2 = torch.tensor([0, 1, 2], dtype=torch.int64)\n",
    "torch_log_probs = torch.nn.functional.log_softmax(torch_t1, dim=1)\n",
    "torch_loss = torch.nn.functional.nll_loss(torch_log_probs, torch_t2, reduction=\"mean\")\n",
    "torch_loss.backward()\n",
    "\n",
    "x = tensor.Tensor([[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], requires_grad=True)\n",
    "y = tensor.Tensor([0, 1, 2])\n",
    "loss_probs = F.log_softmax(x, dim=1)\n",
    "loss = F.nll_loss(loss_probs, y, reduction=\"mean\")\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad, torch_t1.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:05:11.736101Z",
     "start_time": "2023-07-10T08:05:11.732483Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "torch_t1 = torch.tensor(\n",
    "    [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]],\n",
    "    dtype=torch.float64,\n",
    "    requires_grad=True,\n",
    ")\n",
    "torch_t2 = torch.tensor([[0, 1], [0, 1]], dtype=torch.int64)\n",
    "torch_log_probs = torch.nn.functional.log_softmax(torch_t1, dim=2)\n",
    "torch_loss = torch.nn.functional.nll_loss(\n",
    "    torch_log_probs.flatten(0, 1), torch_t2.flatten(), reduction=\"mean\"\n",
    ")\n",
    "torch_loss.backward()\n",
    "\n",
    "x = tensor.Tensor(\n",
    "    [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]], requires_grad=True\n",
    ")\n",
    "y = tensor.Tensor([[0, 1], [0, 1]])\n",
    "loss_probs = F.log_softmax(x, dim=2)\n",
    "loss = F.nll_loss(\n",
    "    loss_probs.reshape((-1, loss_probs.shape[-1])), y.flatten(), reduction=\"mean\"\n",
    ")\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad, torch_t1.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:05:35.499614Z",
     "start_time": "2023-07-10T08:05:35.495778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# backprop over Cross Entropy loss\n",
    "torch_t1 = torch.tensor(\n",
    "    [[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], dtype=torch.float64, requires_grad=True\n",
    ")\n",
    "target = torch.tensor([0, 1, 2], dtype=torch.int64)\n",
    "torch_loss = torch.nn.functional.cross_entropy(torch_t1, target, reduction=\"mean\")\n",
    "torch_loss.backward()\n",
    "\n",
    "x = tensor.Tensor([[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], requires_grad=True)\n",
    "y = tensor.Tensor([0, 1, 2])\n",
    "loss = F.cross_entropy(x, y, reduction=\"mean\")\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad, torch_t1.grad.data.numpy())\n",
    "\n",
    "# backprop over losses as classes: NLLLoss\n",
    "torch_t1 = torch.tensor(\n",
    "    [[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], dtype=torch.float64, requires_grad=True\n",
    ")\n",
    "torch_t2 = torch.tensor([0, 1, 2], dtype=torch.int64)\n",
    "torch_loss = torch.nn.NLLLoss(reduction=\"mean\")(\n",
    "    torch.nn.functional.log_softmax(torch_t1, dim=1), torch_t2\n",
    ")\n",
    "torch_loss.backward()\n",
    "\n",
    "x = tensor.Tensor([[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], requires_grad=True)\n",
    "y = tensor.Tensor([0, 1, 2])\n",
    "loss = nn.NLLLoss(reduction=\"mean\")(F.log_softmax(x, dim=1), y)\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad, torch_t1.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:06:24.312729Z",
     "start_time": "2023-07-10T08:06:24.302809Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# backprop over losses as classes: CrossEntropyLoss\n",
    "torch_t1 = torch.tensor(\n",
    "    [[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], dtype=torch.float64, requires_grad=True\n",
    ")\n",
    "target = torch.tensor([0, 1, 2], dtype=torch.int64)\n",
    "torch_loss = torch.nn.CrossEntropyLoss(reduction=\"mean\")(torch_t1, target)\n",
    "torch_loss.backward()\n",
    "\n",
    "x = tensor.Tensor([[1, 3, 6, 4], [2, 4, 5, 4], [2, 3, 1, 5]], requires_grad=True)\n",
    "y = tensor.Tensor([0, 1, 2])\n",
    "# loss = loss_module.CrossEntropyLoss(reduction=\"mean\")(x, y)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"mean\")(x, y)\n",
    "loss.backward()\n",
    "assert np.allclose(x.grad, torch_t1.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:06:52.095488Z",
     "start_time": "2023-07-10T08:06:52.092571Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#######################\n",
    "# testing matmul and backprop through matmul\n",
    "#######################\n",
    "x = tensor.Tensor([[1, 2], [3, 4]], requires_grad=True)\n",
    "W = tensor.Tensor([[1], [2]], requires_grad=True)\n",
    "target = tensor.Tensor([[4], [8]])\n",
    "loss = F.mse_loss(x @ W, target)\n",
    "loss.backward()\n",
    "\n",
    "torch_x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64, requires_grad=True)\n",
    "torch_W = torch.tensor([[1], [2]], dtype=torch.float64, requires_grad=True)\n",
    "torch_target = torch.tensor([[4], [8]], dtype=torch.float64)\n",
    "torch_loss = torch.nn.functional.mse_loss(torch_x @ torch_W, torch_target)\n",
    "torch_loss.backward()\n",
    "\n",
    "assert np.allclose(x.grad, torch_x.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:07:11.161211Z",
     "start_time": "2023-07-10T08:07:11.157375Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# a tougher Test\n",
    "x = tensor.Tensor(\n",
    "    [\n",
    "        [1, 2, 3, 4],\n",
    "        [2, 3, 4, 5],\n",
    "    ],\n",
    "    requires_grad=True,\n",
    ")\n",
    "W = tensor.Tensor(\n",
    "    [[1, 2, 1, 2, 1], [2, 3, 2, 3, 2], [3, 4, 3, 4, 3], [4, 5, 4, 5, 4]],\n",
    "    requires_grad=True,\n",
    ")\n",
    "target = tensor.Tensor([0, 1], dtype=np.int32)\n",
    "logits = x @ W\n",
    "loss = F.cross_entropy(logits, target)\n",
    "loss.backward()\n",
    "\n",
    "torch_x = torch.tensor(x.data, dtype=torch.float64, requires_grad=True)\n",
    "torch_W = torch.tensor(W.data, dtype=torch.float64, requires_grad=True)\n",
    "torch_target = torch.tensor(target.data, dtype=torch.int64)\n",
    "torch_logits = torch_x @ torch_W\n",
    "torch_loss = torch.nn.functional.cross_entropy(torch_logits, torch_target)\n",
    "torch_loss.backward()\n",
    "\n",
    "assert np.allclose(x.grad, torch_x.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:07:35.434951Z",
     "start_time": "2023-07-10T08:07:35.425966Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# just testing the ReLU\n",
    "x = tensor.Tensor([[2, -1], [-1, 4]], requires_grad=True)\n",
    "relu = nn.ReLU()\n",
    "y = relu(x)\n",
    "torch_x = torch.tensor([[2, -1], [-1, 4]], dtype=torch.float64, requires_grad=True)\n",
    "torch_relu = torch.nn.ReLU()\n",
    "torch_y = torch_relu(torch_x)\n",
    "assert np.allclose(y.data, torch_y.data.numpy())\n",
    "tanh = nn.Tanh()\n",
    "y = tanh(x)\n",
    "torch_tanh = torch.nn.Tanh()\n",
    "torch_y = torch_tanh(torch_x)\n",
    "assert np.allclose(y.data, torch_y.data.numpy())\n",
    "\n",
    "# testing the backward pass of ReLU\n",
    "x = tensor.Tensor([[2, -1], [-1, 4]], requires_grad=True)\n",
    "relu = nn.ReLU()\n",
    "y = relu(x)\n",
    "y.backward()\n",
    "torch_x = torch.tensor([[2, -1], [-1, 4]], dtype=torch.float64, requires_grad=True)\n",
    "torch_relu = torch.nn.ReLU()\n",
    "torch_y = torch_relu(torch_x)\n",
    "torch_y.backward(torch.ones_like(torch_y))\n",
    "assert np.allclose(x.grad, torch_x.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:08:03.289527Z",
     "start_time": "2023-07-10T08:08:03.283665Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# testing the backward pass of Tanh\n",
    "x = tensor.Tensor([[2, -1], [-1, 4]], requires_grad=True)\n",
    "tanh = nn.Tanh()\n",
    "y = tanh(x)\n",
    "y.backward()\n",
    "torch_x = torch.tensor([[2, -1], [-1, 4]], dtype=torch.float64, requires_grad=True)\n",
    "torch_tanh = torch.nn.Tanh()\n",
    "torch_y = torch_tanh(torch_x)\n",
    "torch_y.backward(torch.ones_like(torch_y))\n",
    "assert np.allclose(x.grad, torch_x.grad.data.numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:08:16.326133Z",
     "start_time": "2023-07-10T08:08:16.323012Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward pass: Tensor(0.5306614205736725)\n",
      "after backward pass: Tensor(0.5004159768747417)\n"
     ]
    }
   ],
   "source": [
    "# let's test the backward pass of a simple network\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "X = tensor.Tensor(xs)\n",
    "Y = tensor.Tensor(ys)\n",
    "layer1 = nn.Linear(3, 1, bias=False)\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.SGD(layer1.parameters(), lr=0.01, momentum=0.0)\n",
    "\n",
    "# forward pass\n",
    "out = layer1(X)\n",
    "loss = criterion(out, Y)\n",
    "print(f\"before backward pass: {loss}\")\n",
    "loss.backward()\n",
    "parameters = list(layer1.parameters())\n",
    "optimizer.step()\n",
    "out = layer1(X)\n",
    "loss = criterion(out, Y)\n",
    "print(f\"after backward pass: {loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:08:33.608207Z",
     "start_time": "2023-07-10T08:08:33.602266Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward pass: Tensor(1.0125859935529213)\n",
      "after backward pass: Tensor(1.0120477755862503)\n"
     ]
    }
   ],
   "source": [
    "# on a deeper network\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "X = tensor.Tensor(xs)\n",
    "Y = tensor.Tensor(ys)\n",
    "layer1 = nn.Linear(3, 2, bias=False)\n",
    "relu = nn.ReLU()\n",
    "layer2 = nn.Linear(2, 1, bias=False)\n",
    "criterion = nn.MSELoss(reduction=\"mean\")\n",
    "parameters = list(layer1.parameters()) + list(layer2.parameters())\n",
    "optimizer = optim.SGD(parameters, lr=0.01, momentum=0.0)\n",
    "\n",
    "# forward pass\n",
    "out = layer1(X)\n",
    "out = relu(out)\n",
    "out = layer2(out)\n",
    "loss = criterion(out, Y)\n",
    "print(f\"before backward pass: {loss}\")\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "out = layer1(X)\n",
    "out = relu(out)\n",
    "out = layer2(out)\n",
    "loss = criterion(out, Y)\n",
    "print(f\"after backward pass: {loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-10T08:08:51.861536Z",
     "start_time": "2023-07-10T08:08:51.833615Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
